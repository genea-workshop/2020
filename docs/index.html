<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163300228-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-163300228-1', {"page_path": "/2020/" + location.hash.slice(1) });
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>GENEA workshop 2020</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/iva.min.css" rel="stylesheet">

</head>

<body id="page-top">

  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none">GENEA workshop 2020</span>
      <!-- <span class="d-none d-lg-block">
        <img src="img/avatar.png" class="img-fluid img-profile rounded-circle mx-auto mb-5" alt="">
        
      </span> -->
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#home">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#important-dates">Important dates</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#call-for-papers">Call for papers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#author-instructions">Author instructions</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#gesture-generation-challenge">Gesture generation challenge</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#workshop-programme">Workshop programme</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#organising-committee">Organising committee</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#sponsors">Sponsors</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#data-and-proceedings">Data and proceedings</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container-fluid p-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="home">
      <div class="w-100">
        
        <!-- <div class="row mb-5">
          <div class="col-9">
            
            <div class="row justify-content-md-center">
              
      </div>
      </div>
    </div> -->
        <div class="row">
          <div class="col-9">
        <h1 class="mb-0">GENEA workshop 2020
        </h1>
        
        <div class="subheading mb-5">Generation and Evaluation of Non-verbal Behaviour for Embodied Agents</div>
        <!--  -->
        
        <p class="mb-5">The GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Workshop 2020 aims to bring together researchers working on the generation and evaluation of nonverbal behaviour for social
          robots, virtual agents, or the like. We invite all interested researchers to submit a paper related to their
          work in the area and to participate in the workshop. <br><br>
          Attached to the workshop is a challenge to assess and advance the state of the art in co-speech
          gesture generation, which is detailed <a href="#gesture-generation-challenge">here</a>. <br><br>
          The workshop will take place on September 12, 2020, in Glasgow, Scotland, as an official
          workshop of <a href="https://iva2020.psy.gla.ac.uk/">ACM IVA’20</a>. Detailed location information will be announced when available. <br><br>
          The data collection for the gesture-generation challenge is graciously sponsored by <a href="https://www.vicon.com/">Vicon Motion
            Systems</a>.
            
        </p>
        <div class="row justify-content-center">
          
          <img src="img/avatar.png" class=" mt-2" width=300 alt=""></div>
      </div>
        <div class="col-3">
        <a class="twitter-timeline" href="https://twitter.com/WorkshopGenea?ref_src=twsrc%5Etfw">Tweets by WorkshopGenea</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </div>
    </div>
      </div>
      
    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="important-dates">
      <div class="w-100">
        <h2 class="mb-5">Important dates</h2>

        <!-- <h4>Timeline for regular workshop submissions:</h4>
        <table class="mb-5 date-table"><tr>
          <td><b>22 July</b></td>
          <td><b>Deadline for</b> submission of <b>regular workshop papers</b></td>
        </tr>
        <tr>
          <td>12 Aug.</td>
          <td>Notification of acceptance for regular workshop papers</td>
        </tr>
        <tr>
          <td><b>28 Aug.</b></td>
          <td><b>Camera-ready paper deadline</b></td>
        </tr>
        <tr>
          <td><b>12 Sept.</b></td>
          <td><b>Workshop date</b></td>
        </tr></table>
      
        <h4>Timeline for challenge participants:</h4> -->
        <table class="date-table">
          <tr>
            <td>1st July</td>
            <td>Challenge dataset released to participants</td>
          </tr>
          <!-- <tr>
            <td><b>29 May</b></td>
            <td><b>Deadline for</b> registration for the challenge</td>
          </tr> -->
          <tr>
            <td>7th Aug</td>
            <td>Test inputs released to participants</td>
          </tr>
          <tr>
            <td><b>15th Aug</b></td>
            <td><b>Deadline for</b> participants to submit <b>generated motion</b></td>
          </tr>
          <tr>
            <td>21st Aug</td>
            <td>Crowdsourced evaluation begins</td>
          </tr>
          <tr>
            <td>5th Sept</td>
            <td>Crowdsourced evaluation closes</td>
          </tr>
          <tr>
            <td>9th Sept</td>
            <td>Evaluation results released to participants</td>
          </tr>
          <tr>
            <td><b>14th Sept</b></td>
            <td><b>Deadline for</b> participants to submit <b>workshop papers</b></td>
          </tr>
          <tr>
            <td><b>19th Sept</b></td>
            <td><b>Deadline for</b> participants to submit <b>system-presentation papers</b></td>
          </tr>
          <tr>
            <td>30st Sept</td>
            <td>Notification of acceptance for workshop papers</td>
          </tr>
          <tr>
            <td>1st Oct</td>
            <td>Notification of acceptance for challenge papers</td>
          </tr>
          <tr>
            <td><b>11th Oct</b></td>
            <td><b>Deadline for camera-ready papers</b></td>
          </tr>
          <tr>
            <td><b>18th Oct</b></td>
            <td><b>Workshop date</b></td>
          </tr>
        </table>
      </div>
    </section>


    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="call-for-papers">
      <!-- justify-content-center -->
      <div class="w-100">
        <h2 class="mb-5">Call for papers</h2>

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            <h4>Overview</h4>

            Generating nonverbal behaviour, such as gesticulation and facial expressions, is of great importance for
            natural interaction with embodied agents such as virtual agents and social robots. At present, behaviour
            generation is typically powered by rule-based systems, data-driven approaches, and their hybrids. For
            evaluation, both objective and subjective methods exist, but their application and validity are frequently a
            point of contention in peer review. <br>
            <br>
            This workshop asks “What will be the behaviour-generation methods of the future? And how can we evaluate
            these as meaningfully and usefully as possible?” The aim is to bring together researchers working on the
            generation and evaluation of nonverbal behaviour for embodied agents to discuss the future of these fields.
            To kickstart these discussions, we invite all interested researchers to submit a paper related to their work
            in the area for presentation at the workshop. <br>
            <br>
            Attached to this workshop is a challenge to assess and advance the state of the art in co-speech gesture
            generation, for which a separate call for participation will be released.
            <br><br>
            <h4>Paper topics include (but are not limited to) the following</h4>
            
            <ul>
              <li>Co-speech gesture generation</li>
              <li>Nonverbal feedback</li>
              <li>Interactive nonverbal behaviour generation</li>
              <li>Evaluation methods for generated nonverbal behaviour</li>
              <li>Objective evaluation metrics for nonverbal behaviour</li>
              <li>Guidelines for nonverbal behaviours in human-agent interaction</li>
            </ul>

          </div>
          <!-- <div class="iva-date text-md-right">
            <span class="text-primary">March 2013 - Present</span>
          </div> -->
        </div>



      </div>

    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="author-instructions">
      <div class="w-100">
        <h2 class="mb-5">Author instructions</h2>

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            Please format your workshop submissions for double-blind review according to the conference template:
            <a href="https://github.com/genea-workshop/2020/raw/master/docs/downloads/genea_template.zip">genea_template.zip</a>

             <br>
            The same rules and guidelines regarding double-blind reviewing as for the main IVA conference apply.
            Submissions should represent original, unpublished work or extensions thereof.<br>
            <br>
            Papers are limited to 5 pages, not counting references and full-page figures. The maximum file size is 20
            MB. Anonymous video sharing through FigShare is strongly encouraged. Submissions should be made in PDF
            format through EasyChair: <br> <a href="https://easychair.org/conferences/?conf=geneaworkshop2020">https://easychair.org/conferences/?conf=geneaworkshop2020</a><br>
            <br>
            Please contact the organisers at <a href="mailto:genea-contact@googlegroups.com">genea-contact@googlegroups.com</a> if you have any questions.<br>

          </div>


        </div>
    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="gesture-generation-challenge">
      <div class="w-100">
        <h2>Gesture generation challenge</h2>
        

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            <h4>Call for participation</h4>
            The state of the art in co-speech gesture generation is difficult to assess, since every research group
            tends to use their own data, embodiment, and evaluation methodology. To better understand and compare
            methods for gesture generation and evaluation, we are launching a new challenge – the <b>
              GENEA (Generation and
              Evaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2020
            </b> – wherein different
            gesture-generation approaches are evaluated side by side in a large user study. The results of the challenge
            will be presented at the GENEA workshop.<br>
            <br>
            We invite researchers in academia and industry working on any form of corpus-based nonverbal behaviour
            generation and gesticulation to submit entries to the challenge, whether their method is driven by rule or
            machine learning. Participants are provided a large, common dataset of speech (audio+aligned text) and 3D
            motion to develop their systems, and then use these systems to generate motion on given test inputs. The
            generated motion clips are rendered onto a common virtual agent and evaluated for motion quality and
            appropriateness in a crowdsourced user study. Additional details are provided below and in the <a href="#rules">rules of
            participation</a>.<br><br>
            <h4>Registration</h4>
            To take part in the challenge, please sign up here (<b>Deadline for registration for the challenge 29 May</b>):<br>
            
            <a href="https://forms.gle/umHTNGja94UPvZRr8">https://forms.gle/umHTNGja94UPvZRr8</a><br>
            
            <br>
            Please make sure to read and agree to the <a href="#rules">challenge rules</a> before registering. Do not register for the
            challenge if you do not intend to comply with the rules.<br><br>
            <h4>Data</h4>
            The challenge data is only available to registered participants. Access to the data requires completing a
            license agreement, which will be distributed to participants via e-mail after registering for
            participation.<br>
            <br>
            Access to the test inputs will also be provided via e-mail. These materials additionally include a
            questionnaire to be completed and included with your submission.<br><br>
            <h4>
              Paper submission and timeline
            </h4>
            Challenge participants are invited to describe their systems and findings in a paper to be presented at the
            workshop. Accepted papers will be published in proceedings on the workshop website and on Zenodo.<br>
            <br>
            Papers describing challenge systems use the same template as regular workshop papers, and are subject to the
            same length and size restrictions, except that:<br>
            <ol>
              <li>The deadlines are different, to allow time for evaluation before completing the paper. Please refer to the
                <a href="#important-dates">important dates for the timeline that applies to challenge participants.</a></li>
              <li>Submissions use camera-ready formatting and author names are not blinded for review.</li>
            </ol>
            
            
            Please contact the organisers at <a href="mailto:genea-contact@googlegroups.com">genea-contact@googlegroups.com</a> if you have any questions.<br>
            <br>
            <hr>

            <h3 class="mt-5" id="rules">Rules for the GENEA Challenge 2020</h3>
Only register for this challenge if you actually intend to submit an entry to the challenge and to comply with all its rules.<br><br>
<h4>
  Database access
</h4>
The gesture database is currently only available to registered participants in the challenge. Access to the data also requires completing and agreeing to the data license agreement. <br>
Download passwords will be issued after your registration is accepted and you have completed the required licenses. <br><br>

<h4>
  Materials provided
</h4>
All participants who have signed the license will be given access to the following materials: <br>
<ul>
  <li>3D full-body motion-capture clips of a speaking and gesticulating person, in BVH format.</li>
  <li>Aligned audio waveforms of the speech associated with the motion-capture clips, in WAV format.</li>
  <li>Text transcripts of each audio file with word-level timing information, in TXT and JSON format</li>
  <li>A number coding the identity of the person gesticulating in each recording (if the data contains motion from multiple persons).</li>
  <li>Code and scripts for replicating the training of the previously-published baseline systems to be included in the challenge evaluation (once available).</li>
  <li>A pipeline for visualising their system output as videos of a gesticulating avatar, the same as will be used to render videos for the challenge evaluation.</li>
</ul>
Prior to the full data release to participants, dummy data files illustrating the format illustrating the folder structure, filenames, and data formats will be available. This allows participants to set up their data-processing pipelines in advance.<br><br>

Approximately one week before the deadline to submit generated motion stimuli, participants will also be given access to:
<ul>
  <li>Held-out audio waveforms from the same source as the training audio, in WAV format.</li>
  <li>Text transcripts of each held-out audio file with word-level timing information, in TXT format.</li>
  <li>A number coding the identity of the person gesticulating in each recording (if the data contains motion from multiple persons).</li>
</ul>

The task of the challenge is to use one’s system to generate convincing gesture motion for this held-out speech. Not all of the synthetic motion output may be included in the final evaluation. <br>

While we endeavour for participants to be able to retain and keep using the challenge data for future research, the extent to which this is possible (if at all) is governed by the license agreement that participants sign.<br>

If, for some reason, you have or gain access to the held-out motion data, we rely on your honesty in not looking at that material or letting it influence your challenge submission.<br><br>
<h4>
  Limits on participation
</h4>
Each participating team may only submit one system per team for evaluation. Teams can consist of one or more persons, from zero or more academic institutions and/or commercial entities.<br><br>

Participants involved in joint projects or consortia who wish to submit multiple systems (e.g., an individual entry and a joint system) should contact the organisers in advance and receive approval first. We will try to accommodate all reasonable requests, provided the evaluation remains manageable. If the number of participating teams is small (e.g., less than five), the organisers may decide to permit multiple entries per team.<br><br>
<h4>
  Use of external data
</h4>
“External data” is defined as data, of any type, that is not part of the provided database. This includes, for example, raw recordings, structured databases, and pre-trained systems such as word vectors.
For this year's challenge, only open external data – data that is available to the public free of charge (possibly after signing a license) – may be used.
All external data used in your system must be explicitly listed by providing a citation and/or link in the paper accompanying your submission.
You are allowed to use external data in any way you wish, subject to any exclusions or limitations given in these rules.

For data pertaining to text and speech, any external data may be used, as long as they satisfy the criteria above. There is no limitation on the amount of such data you may use.<br><br>

For motion data (whether 2D, 3D, or video), only external data from very specific databases may be used in creating your challenge entries. These resources are linked and listed below:
<ul>
  <li>CMU Motion Capture Database: <a href="http://mocap.cs.cmu.edu/">http://mocap.cs.cmu.edu/</a></li>
  <li>Motion Capture Database HDM05: <a href="http://resources.mpi-inf.mpg.de/HDM05/">http://resources.mpi-inf.mpg.de/HDM05/</a></li>
  <li>CMU Panoptic Studio dataset: <a href="http://domedb.perception.cs.cmu.edu/">http://domedb.perception.cs.cmu.edu/</a></li>
</ul>

The reason for this data restriction is that other behaviour-generation challenges have found that system performance often is limited by the amount of training data that can be ingested, which is not an interesting scientific conclusion to replicate.<br><br>

Your system must make use of the provided motion data, but you may exclude parts of that data if you wish. Use of the provided audio or text transcripts is entirely optional and not compulsory. The same applies to the use of external data.<br><br>

Please keep in mind that the point of the challenge is to gain better insight into the synthesis and perception of motion and gestures, not to see who has the best data and resources. Consequently, participants are strongly encouraged to share processed material they are using in their entries with other participants and with the organisers. Example data that may be valuable to share include: improved transcripts and alignments; motions from permitted external databases converted to the challenge format and retargeted to the challenge skeleton; denoised and reconstructed motion data; sub-selected data; bug fixes to baseline systems; etc.<br><br>

If you are in any doubt about how to apply these rules, please contact the organisers for clarification!<br><br>
<h4>
  Synthesising test motion
</h4>
Synthetic gesture motion must be submitted in the same format as that used by the challenge gesture database (BVH, same skeleton, frame rate, etc.). The organisers take no responsibility for any effects that may occur when processing motion that was not submitted in the correct format. <br><br>

To prevent optimising for the specific evaluation used in the challenge, the exact nature of the test set will not be revealed in advance. Manually tweaking the output motion is not allowed, since the idea is to evaluate how systems would perform in an unattended setting.<br><br>
<h4>
  Retention and distribution of submitted stimuli
</h4>
Any stimuli that you submit for evaluation will be retained by the organisers for future use. The evaluated stimuli and any associated user ratings and comments will also be made publicly available for non-commercial purposes, labelled by the corresponding anonymised system label.<br><br>
<h4>
  Evaluation
</h4>
A large formal evaluation by means of a user study will be conducted to jointly evaluate and compare the submitted co-speech gestures. This user study will be carried out online using crowdsourced raters who speak and comprehend the language featured in the database.<br><br>

The evaluation will likely consider aspects such as the human-likeness of the generated gesture motion, its appropriateness (in terms of timing, semantic content, or both) for the associated held-out speech, and its appropriateness for the individual gesticulation style of the test speaker or speakers.<br><br>

Aside from stimuli based on motion submitted by challenge participants, the evaluation will also incorporate motion generated from a handful of baseline approaches based on public code and shared with challenge participants; stimuli based on natural speech and motion; and checks on raters’ attention.<br><br>

The results of the evaluation, including a statistical analysis, will be made public, albeit with the identity of participating systems anonymised. Participating teams will be informed of the results and which system is theirs, so that they can draw conclusions and describe what they learned in papers describing their submissions.<br><br>
<h4>
  Paper
</h4>
<ul>
  <li>
    Each participant <b>must</b> submit a paper (using the template specified) describing their entry for review.
    <ul>
    <li>If you are unable to comply with this requirement, do not enter the challenge!</li>
  </ul></li>
  <li>Papers should describe the system, as well as:
    <ul>
      <li>external data used, if any (e.g., speech and text corpora, word embeddings, etc.);</li>
      <li>any other existing tools, software and models used;</li>
      <li>any manual interventions such as additional data annotation;</li>
      <li>participants’ scientific and engineering takeaway messages from their participation.</li>
      <li>In addition, describing and analysing the results of other evaluations performed, including formal and informal tests (e.g., ablations) as part of the system development, is also strongly encouraged.</li>
    </ul>
  </li>
  <li>Although submitted systems will be anonymised in the challenge results published by the organisers, participants are encouraged to report which anonymised label is associated with their system in their paper and any other publications based on their challenge submission.</li>
  <li>Each participant is also expected to complete a form giving the general technical specification of their system, to facilitate easy cross-system comparisons. (For example Is the motion based on playback such as motion graphs, continuous motion generation, or a hybrid approach? Is the output deterministic or stochastic? Does the system use text input, audio input, or both? Does it make use of external motion data? What computational resources were required to create/train the system? etc.)</li>
  <li>One of the <b>authors</b> of each accepted paper <b>must</b> register and present the paper at the workshop associated with the challenge.
    <ul>
      <li>If you are unable to comply with this requirement, do not enter the challenge!</li>
    </ul>
  </li>
  <li>There is no penalty for dropping out of the challenge prior to the start of the evaluation, other than that the data license might restrict your future use of the challenge data. However, teams whose stimuli are included in the evaluation are required to submit a paper on their system and present it at the workshop.</li>
</ul>
<h4>
  Use of results
</h4>
This gesture generation challenge is a scientific exercise. You may use the results only for the purpose of scientific research. Specifically, you may <b>not</b> use the results (e.g., your team’s ranking in the evaluations) for any commercial purposes, including but not limited to advertising products or services. <br><br>
<h4>
  How are these rules enforced?
</h4>
This is a challenge, which is designed to advance scientific knowledge, and not a competition. The point is not to find who does best, but what works best. Therefore, we depend on your honesty in preparing your entry.

          </div>


        </div>

    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="workshop-programme">
      <div class="w-100">
        <h2 class="mb-5">Tentative workshop programme (subject to change):</h2>
        <table class="programme-date-table">
          <tr>
            <td>09:00–09:10</td>
            <td>Opening remarks</td>
          </tr>
          <tr>
            <td>09:10–10:00</td>
            <td>Keynote presentation (speaker TBD)</td>
          </tr>
          <tr>
            <td>10:00–10:15</td>
            <td>Coffee break</td>
          </tr>
          <tr>
            <td>10:15–10:25</td>
            <td>Introduction to the challenge</td>
          </tr>
          <tr>
            <td>10:30–12:00</td>
            <td>Presentations of challenge submissions</td>
          </tr>
          <tr>
            <td>12:00–13:30</td>
            <td>Lunch break</td>
          </tr>
          <tr>
            <td>13:30–14:30</td>
            <td>Workshop paper spotlights</td>
          </tr>
          <tr>
            <td>14:30–16:00</td>
            <td>Poster session with coffee</td>
          </tr>
          <tr>
            <td>16:00–17:00</td>
            <td>Group discussions</td>
          </tr>
          <tr>
            <td>17:00–17:20</td>
            <td>Closing remarks</td>
          </tr>
          <tr>
            <td>17:20–18:00</td>
            <td>Discussion about the future</td>
          </tr>
          <tr>
            <td>Evening</td>
            <td>Voluntary social event at nearby pub</td>
          </tr>
        </table>
      </div>
    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="organising-committee">
      <div class="w-100">
        <h2 class="mb-5">Organising committee</h2>
        <p>
          The main contact address of the workshop is: <a href="mailto:genea-contact@googlegroups.com">genea-contact@googlegroups.com</a>. <br> <br>

          <h4>Workshop organisers</h4>
          <table class="w-100 table">
            <tr>
              <td><img src="img/taras.jpg" alt="Taras Kucherenko" width="150" style="border-radius: 5px"> </td>
              <td><a href="https://svito-zar.github.io/" style="font-weight: bold;">Taras Kucherenko</a>​ <br>
                KTH Royal Institute of Technology <br> Sweden
              </td>
              <td><img src="img/gustav.jpeg" alt="Gustav Eje Henter​" width="150" style="border-radius: 5px"></td>
              <td><b>Gustav Eje Henter​</b> <br>KTH Royal Institute of Technology <br> Sweden</td>
            </tr>
            
            <tr>
              <td><img src="img/pieter.jpeg" alt="Pieter Wolfert​" width="150" style="border-radius: 5px"></td>
              <td><a href="https://www.pieterwolfert.com" style="font-weight: bold;">Pieter Wolfert​</a> <br>IDLab, Ghent University - imec <br> Belgium </td>
              <td><img src="img/youngwoo.jpg" alt="Youngwoo Yoon" width="150" style="border-radius: 5px"></td>
              <td><a href="https://sites.google.com/view/youngwoo-yoon/" style="font-weight: bold;">Youngwoo Yoon</a> <br>ETRI & KAIST <br> South Korea </td>
            </tr>
            <tr style="border-bottom: 1px solid rgb(222, 226, 230)">
              <td><img src="img/patrik.jpg" alt="Patrik Jonell" width="150" style="border-radius: 5px"></td>
              <td><a href="https://www.patrikjonell.se" style="font-weight: bold;">Patrik Jonell</a> <br>KTH Royal Institute of Technology <br> Sweden </td>
              <td><img src="img/ulysses.jpg" alt="Ulysses Bernardet" width="150" style="border-radius: 5px"></td>
              <td><b>Ulysses Bernardet</b> <br> Aston University <br> UK</td>
            </tr>
          </table>

          
          
          
          
          
          

        </p>
      </div>
    </section>

    <hr class="m-0">


    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="sponsors">
      <div class="w-100">
        <h2 class="mb-5">Sponsors</h2>
        <p>
          The data collection for the gesture-generation challenge is graciously sponsored by <a href="https://www.vicon.com/">Vicon Motion Systems</a>. <br><br>
          Additional sponsors will be announced shortly. <br> <br>

          If you are interested in sponsoring the GENEA workshop or challenge, please contact the organisers at <a href="mailto:genea-contact@googlegroups.com">genea-contact@googlegroups.com</a>.


        </p>
      </div>
    </section>

    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="data-and-proceedings">
      <div class="w-100">
        <h2 class="mb-5">Data and proceedings</h2>
        <p>
          Conference proceedings (pdf files of accepted papers) will be made permanently available to the public through
          <a href="https://zenodo.org/">Zenodo</a>. Outputs from the challenge, such as input text and audio, BVH motion, rendered videos, and their
          subjective ratings, will also be made available there.
        </p>
      </div>
    </section>

    <hr class="m-0">


  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/iva.min.js"></script>

</body>

</html>